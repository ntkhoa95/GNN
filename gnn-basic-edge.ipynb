{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Niko' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Graph Neural Network with Edge Prediction on MNIST\n",
    "\n",
    "A very basic neural network (GNN) implementation using a subnetwork for edge prediction.\n",
    "\n",
    "<br>Here, the 28x28 image of a digit in MNIST represents the graph, where each pixel (i.e., cell in the grid) represents a particular node. The feature of that node is simply the pixel intensity in range [0, 1].\n",
    "\n",
    "<br>The adjacency matrix of the pixels was basically just determined by the neighborhood pixels. Using a Gaussian filter, pixels were connected based on their Euclidean distance in grid. In this notebook, the edges are predicted via a seperate neural network model.\n",
    "\n",
    "<br>\n",
    "self.pred_edge_fc = nn.Sequential(nn.Linear(coord_features, 64), \n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(64, 1),\n",
    "                                    nn.Tanh())\n",
    "\n",
    "<br>Using the resulting adjacency matrix A, we can compute the output of a layer as\n",
    "\\begin{align*}\n",
    "X^{(l+1)} = AX^{(l)}W^{(l)}\n",
    "\\end{align*}\n",
    "\n",
    "<br>Here, A is the NxN adjacency matrix, and X is the NxC feature matrix (a 2D coordinate array, where N is the total number of pixels -- 28x28=784 in MNIST). W is the weight matrix of shape NxP, where P would represent the number of classes if we have only a single hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time, torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Setting hyper-parameters\n",
    "RANDOM_SEED = 42\n",
    "LAERNING_RATE = 0.0005\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=custom_transform, download=True)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, transform=custom_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, std = torch.zeros(3), torch.zeros(3)\n",
    "# for img, label in train_dataset:\n",
    "#     mean += torch.mean(img, dim=(1, 2))\n",
    "#     std  += torch.std(img, dim=(1, 2))\n",
    "\n",
    "# mean /= len(train_dataset)\n",
    "# std  /= len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.9 # 90% for training, 10% for valid\n",
    "train_samples_num = int(len(train_dataset) * SPLIT_RATIO)\n",
    "val_samples_num = len(train_dataset) - train_samples_num\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_samples_num, val_samples_num])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
    "val_loader   = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)\n",
    "test_loader  = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)\n",
    "\n",
    "# Check the dataset\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimension: ', images.shape)\n",
    "    print('Image label dimension: ', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coordinate_array(img_size, out_size=4):\n",
    "    # Make 2D coordinate array (for MNIST 784x2)\n",
    "    n_rows = img_size * img_size\n",
    "    col, row = np.meshgrid(np.arange(img_size), np.arange(img_size))\n",
    "    coord = np.stack((col, row), axis=2).reshape(-1, 2)\n",
    "    coord = (coord - np.mean(coord, axis=0)) / (np.std(coord, axis=0) + 1e-5)\n",
    "    coord = torch.from_numpy(coord).float()\n",
    "\n",
    "    # Reshape to [N, N, out_size]\n",
    "    coord = torch.cat((coord.unsqueeze(0).repeat(n_rows, 1, int(out_size/2-1)), \n",
    "                        coord.unsqueeze(1).repeat(1, n_rows, 1)), dim=2)\n",
    "    return coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Model\n",
    "\n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, img_size=28, coord_features=4, num_classes=10):\n",
    "        super(GraphNet, self).__init__()\n",
    "\n",
    "        n_rows = img_size**2\n",
    "        self.fc = nn.Linear(n_rows, num_classes, bias=False)\n",
    "        coord = make_coordinate_array(img_size, coord_features)\n",
    "        self.register_buffer('coord', coord)\n",
    "\n",
    "        # Edge Predictor\n",
    "        self.pred_edge_fc = nn.Sequential(nn.Linear(coord_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 1),\n",
    "                                            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "\n",
    "        # Predict edges\n",
    "        self.A = self.pred_edge_fc(self.coord).squeeze()\n",
    "\n",
    "        # Reshape Adjacency Matrix\n",
    "        # [N, N] Adjacency Matrix -> [1, N, N] Adjacency tensor where N = H x W\n",
    "        A_tensor = self.A.unsqueeze(0)\n",
    "        A_tensor = A_tensor.expand(B, -1, -1) # Bx784x784\n",
    "\n",
    "        # Reshape inputs\n",
    "        # [B, C, H, W] -> [B, HxW, 1]\n",
    "        x_reshape = x.view(B, -1, 1) # Bx784x1\n",
    "\n",
    "        # bmm: batch matrix product to sum the neighbor features\n",
    "        # Input: [B, N, N] x [B, N, 1]\n",
    "        # Output: [B, N]\n",
    "        avg_neighbor_features = torch.bmm(A_tensor, x_reshape).view(B, -1) # Bx784\n",
    "        logits = self.fc(avg_neighbor_features)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = GraphNet(img_size=IMG_SIZE, num_classes=10)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(model, dataloader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for features, targets in dataloader:\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "        _, pred_labels = torch.max(probas, 1)\n",
    "        num_examples += features.size(0)\n",
    "        correct_pred += (pred_labels == targets).sum()\n",
    "    return correct_pred.float() / num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "loss_list = []\n",
    "train_acc_list, val_acc_list, test_acc_list = [], [], []\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(DEVICE)\n",
    "        targets  = targets.to(DEVICE)\n",
    "\n",
    "        # Forward and back-propagation\n",
    "        logits, probas = model(features)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        if not batch_idx % 150:\n",
    "            print(f\"Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} |\"\n",
    "                    f\"Batch: {batch_idx:03d}/{len(train_loader):03d} |\"\n",
    "                    f\"Loss: {loss:.4f}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_acc = compute_acc(model, train_loader, DEVICE)\n",
    "        val_acc   = compute_acc(model, val_loader, DEVICE)\n",
    "        test_acc  = compute_acc(model, test_loader, DEVICE)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d}\\n\"\n",
    "                f\"Train Accuracy: {train_acc:.2f} | Valid Accuracy: {val_acc:.2f} | Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"Time elapsed: {elapsed:.2f} min\")\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"Total Training Time: {elapsed:.2f} min\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list, label='Loss')\n",
    "plt.plot(np.convolve(loss_list, np.ones(200,) / 200, mode='valid'), label='Running Average')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, NUM_EPOCHS+1), train_acc_list, label='Training')\n",
    "plt.plot(np.arange(1, NUM_EPOCHS+1), val_acc_list, label='Validation')\n",
    "plt.plot(np.arange(1, NUM_EPOCHS+1), test_acc_list, label='Testing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.A.cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd409492fea0b90f1e24a4fd0f416e2edfe4fabf2ed7f769ce0dd2938f1b1517"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
